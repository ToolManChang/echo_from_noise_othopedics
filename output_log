Many modules are hidden in this stack. Use "module --show_hidden spider SOFTWARE" if you are not able to find the required software
[2025-01-02 17:43:25,376] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-02 17:43:25,376] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-02 17:43:25,376] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-02 17:43:25,376] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-02 17:43:32,015] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-01-02 17:43:32,015] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-01-02 17:43:32,015] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-01-02 17:43:32,015] [INFO] [comm.py:609:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-01-02 17:43:32,015] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-01-02 17:43:32,015] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-01-02 17:43:32,015] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-01-02 17:43:32,015] [INFO] [comm.py:609:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-01-02 17:43:32,015] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2025-01-02 17:43:32,015] [INFO] [comm.py:609:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-01-02 17:43:32,016] [INFO] [comm.py:594:init_distributed] cdb=None
[2025-01-02 17:43:32,016] [INFO] [comm.py:609:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-01-02 17:43:32,054] [INFO] [comm.py:659:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=4, master_addr=10.204.97.11, master_port=29500
[2025-01-02 17:43:32,054] [INFO] [comm.py:659:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=4, master_addr=10.204.97.11, master_port=29500
[2025-01-02 17:43:32,054] [INFO] [comm.py:659:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=4, master_addr=10.204.97.11, master_port=29500
[2025-01-02 17:43:32,054] [INFO] [comm.py:659:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=4, master_addr=10.204.97.11, master_port=29500
[2025-01-02 17:43:32,054] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Logging to /cluster/project/krause/yunkao/echo_from_noise/output
creating model and diffusion...
creating data loader...
training...
Len of Dataset: 2000
Len of Dataset: 2000
Len of Dataset: 2000
Len of Dataset: 2000
--------------------------------
| grad_norm       | 4.17858    |
| lg_loss_scale   | 20         |
| loss            | 1.00056    |
| loss_q0         | 0.999529   |
| loss_q1         | 1.00066    |
| loss_q2         | 1.00202    |
| loss_q3         | 1.00005    |
| lr              | 0.0001     |
| lr_anneal_steps | 50000      |
| mse             | 1.00055    |
| mse_q0          | 0.999512   |
| mse_q1          | 1.00065    |
| mse_q2          | 1.00202    |
| mse_q3          | 1.00002    |
| param_norm      | 328.967    |
| samples         | 48         |
| step            | 0          |
| vb              | 0.013319   |
| vb_q0           | 0.017612   |
| vb_q1           | 0.00520767 |
| vb_q2           | 0.00477293 |
| vb_q3           | 0.0256834  |
--------------------------------
--------------------------------
| grad_norm       | 4.19664    |
| lg_loss_scale   | 20.0055    |
| loss            | 0.818171   |
| loss_q0         | 0.840358   |
| loss_q1         | 0.807103   |
| loss_q2         | 0.825419   |
| loss_q3         | 0.796316   |
| lr              | 9.998e-05  |
| lr_anneal_steps | 50000      |
| mse             | 0.818153   |
| mse_q0          | 0.840341   |
| mse_q1          | 0.8071     |
| mse_q2          | 0.825415   |
| mse_q3          | 0.796274   |
| param_norm      | 328.993    |
| samples         | 528        |
| step            | 10         |
| vb              | 0.0174798  |
| vb_q0           | 0.0167467  |
| vb_q1           | 0.00387995 |
| vb_q2           | 0.00420921 |
| vb_q3           | 0.0421577  |
--------------------------------
--------------------------------
| grad_norm       | 3.39614    |
| lg_loss_scale   | 20.0155    |
| loss            | 0.474283   |
| loss_q0         | 0.549449   |
| loss_q1         | 0.44106    |
| loss_q2         | 0.458525   |
| loss_q3         | 0.458222   |
| lr              | 9.996e-05  |
| lr_anneal_steps | 50000      |
| mse             | 0.474274   |
| mse_q0          | 0.549428   |
| mse_q1          | 0.441058   |
| mse_q2          | 0.458523   |
| mse_q3          | 0.458213   |
| param_norm      | 329.094    |
| samples         | 1008       |
| step            | 20         |
| vb              | 0.0084972  |
| vb_q0           | 0.0214787  |
| vb_q1           | 0.00224425 |
| vb_q2           | 0.00232326 |
| vb_q3           | 0.00947653 |
--------------------------------
--------------------------------
| grad_norm       | 2.37521    |
| lg_loss_scale   | 20.0255    |
| loss            | 0.231666   |
| loss_q0         | 0.324873   |
| loss_q1         | 0.226657   |
| loss_q2         | 0.207525   |
| loss_q3         | 0.196537   |
| lr              | 9.994e-05  |
| lr_anneal_steps | 50000      |
| mse             | 0.231663   |
| mse_q0          | 0.324865   |
| mse_q1          | 0.226656   |
| mse_q2          | 0.207524   |
| mse_q3          | 0.196533   |
| param_norm      | 329.207    |
| samples         | 1488       |
| step            | 30         |
| vb              | 0.00305735 |
| vb_q0           | 0.00712176 |
| vb_q1           | 0.00113042 |
| vb_q2           | 0.00107443 |
| vb_q3           | 0.00443134 |
--------------------------------
---------------------------------
| grad_norm       | 1.56486     |
| lg_loss_scale   | 20.0355     |
| loss            | 0.112036    |
| loss_q0         | 0.208485    |
| loss_q1         | 0.0915912   |
| loss_q2         | 0.0769483   |
| loss_q3         | 0.0769591   |
| lr              | 9.992e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.11203     |
| mse_q0          | 0.208462    |
| mse_q1          | 0.0915907   |
| mse_q2          | 0.076948    |
| mse_q3          | 0.0769573   |
| param_norm      | 329.33      |
| samples         | 1968        |
| step            | 40          |
| vb              | 0.00624511  |
| vb_q0           | 0.0237839   |
| vb_q1           | 0.000458524 |
| vb_q2           | 0.000376259 |
| vb_q3           | 0.00181022  |
---------------------------------
---------------------------------
| grad_norm       | 0.87187     |
| lg_loss_scale   | 20.0455     |
| loss            | 0.0489157   |
| loss_q0         | 0.119223    |
| loss_q1         | 0.0392784   |
| loss_q2         | 0.0267074   |
| loss_q3         | 0.0235266   |
| lr              | 9.99e-05    |
| lr_anneal_steps | 50000       |
| mse             | 0.0489112   |
| mse_q0          | 0.119203    |
| mse_q1          | 0.0392782   |
| mse_q2          | 0.0267073   |
| mse_q3          | 0.023526    |
| param_norm      | 329.45      |
| samples         | 2448        |
| step            | 50          |
| vb              | 0.00443468  |
| vb_q0           | 0.0192412   |
| vb_q1           | 0.000206316 |
| vb_q2           | 0.000139388 |
| vb_q3           | 0.000649868 |
---------------------------------
---------------------------------
| grad_norm       | 0.579403    |
| lg_loss_scale   | 20.0555     |
| loss            | 0.035299    |
| loss_q0         | 0.105703    |
| loss_q1         | 0.0266115   |
| loss_q2         | 0.0139847   |
| loss_q3         | 0.0101461   |
| lr              | 9.988e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0352974   |
| mse_q0          | 0.105697    |
| mse_q1          | 0.0266114   |
| mse_q2          | 0.0139846   |
| mse_q3          | 0.0101452   |
| param_norm      | 329.554     |
| samples         | 2928        |
| step            | 60          |
| vb              | 0.00159905  |
| vb_q0           | 0.0061925   |
| vb_q1           | 0.000143283 |
| vb_q2           | 7.27021e-05 |
| vb_q3           | 0.000920557 |
---------------------------------
---------------------------------
| grad_norm       | 0.377261    |
| lg_loss_scale   | 20.0655     |
| loss            | 0.0308714   |
| loss_q0         | 0.0795887   |
| loss_q1         | 0.0223408   |
| loss_q2         | 0.0115947   |
| loss_q3         | 0.00774777  |
| lr              | 9.986e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0308266   |
| mse_q0          | 0.0795868   |
| mse_q1          | 0.0223407   |
| mse_q2          | 0.0115947   |
| mse_q3          | 0.00760031  |
| param_norm      | 329.666     |
| samples         | 3408        |
| step            | 70          |
| vb              | 0.04478     |
| vb_q0           | 0.00193734  |
| vb_q1           | 0.000118166 |
| vb_q2           | 5.87297e-05 |
| vb_q3           | 0.147459    |
---------------------------------
---------------------------------
| grad_norm       | 0.31684     |
| lg_loss_scale   | 20.0755     |
| loss            | 0.027931    |
| loss_q0         | 0.0726467   |
| loss_q1         | 0.0210675   |
| loss_q2         | 0.0100475   |
| loss_q3         | 0.00663633  |
| lr              | 9.984e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0279303   |
| mse_q0          | 0.0726443   |
| mse_q1          | 0.0210674   |
| mse_q2          | 0.0100475   |
| mse_q3          | 0.00663618  |
| param_norm      | 329.785     |
| samples         | 3888        |
| step            | 80          |
| vb              | 0.000691666 |
| vb_q0           | 0.00239382  |
| vb_q1           | 0.000114269 |
| vb_q2           | 5.11328e-05 |
| vb_q3           | 0.000145056 |
---------------------------------
---------------------------------
| grad_norm       | 0.522052    |
| lg_loss_scale   | 20.0855     |
| loss            | 0.0330659   |
| loss_q0         | 0.10897     |
| loss_q1         | 0.0204593   |
| loss_q2         | 0.00892455  |
| loss_q3         | 0.00609534  |
| lr              | 9.982e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.033063    |
| mse_q0          | 0.108958    |
| mse_q1          | 0.0204592   |
| mse_q2          | 0.0089245   |
| mse_q3          | 0.00609505  |
| param_norm      | 329.876     |
| samples         | 4368        |
| step            | 90          |
| vb              | 0.00287702  |
| vb_q0           | 0.0127556   |
| vb_q1           | 0.000111585 |
| vb_q2           | 4.52219e-05 |
| vb_q3           | 0.000283727 |
---------------------------------
---------------------------------
| grad_norm       | 0.411544    |
| lg_loss_scale   | 20.0955     |
| loss            | 0.0393061   |
| loss_q0         | 0.103958    |
| loss_q1         | 0.0173423   |
| loss_q2         | 0.00839456  |
| loss_q3         | 0.00573902  |
| lr              | 9.98e-05    |
| lr_anneal_steps | 50000       |
| mse             | 0.0392561   |
| mse_q0          | 0.103919    |
| mse_q1          | 0.0173422   |
| mse_q2          | 0.00839451  |
| mse_q3          | 0.00558186  |
| param_norm      | 329.949     |
| samples         | 4848        |
| step            | 100         |
| vb              | 0.050012    |
| vb_q0           | 0.0389262   |
| vb_q1           | 8.85128e-05 |
| vb_q2           | 4.22349e-05 |
| vb_q3           | 0.157158    |
---------------------------------
---------------------------------
| grad_norm       | 0.462554    |
| lg_loss_scale   | 20.1055     |
| loss            | 0.0245104   |
| loss_q0         | 0.0826691   |
| loss_q1         | 0.018282    |
| loss_q2         | 0.00823402  |
| loss_q3         | 0.00506111  |
| lr              | 9.978e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0245094   |
| mse_q0          | 0.0826645   |
| mse_q1          | 0.0182819   |
| mse_q2          | 0.00823398  |
| mse_q3          | 0.005061    |
| param_norm      | 330.015     |
| samples         | 5328        |
| step            | 110         |
| vb              | 0.000942858 |
| vb_q0           | 0.00455378  |
| vb_q1           | 9.6155e-05  |
| vb_q2           | 4.08598e-05 |
| vb_q3           | 0.000105384 |
---------------------------------
---------------------------------
| grad_norm       | 0.333607    |
| lg_loss_scale   | 20.1155     |
| loss            | 0.0226039   |
| loss_q0         | 0.0624508   |
| loss_q1         | 0.0169276   |
| loss_q2         | 0.00790943  |
| loss_q3         | 0.00494597  |
| lr              | 9.976e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0226028   |
| mse_q0          | 0.0624463   |
| mse_q1          | 0.0169275   |
| mse_q2          | 0.00790939  |
| mse_q3          | 0.00494589  |
| param_norm      | 330.076     |
| samples         | 5808        |
| step            | 120         |
| vb              | 0.00112783  |
| vb_q0           | 0.00444717  |
| vb_q1           | 8.63986e-05 |
| vb_q2           | 3.96193e-05 |
| vb_q3           | 8.50669e-05 |
---------------------------------
---------------------------------
| grad_norm       | 0.244367    |
| lg_loss_scale   | 20.1255     |
| loss            | 0.0203947   |
| loss_q0         | 0.0543412   |
| loss_q1         | 0.0176214   |
| loss_q2         | 0.0072954   |
| loss_q3         | 0.00458403  |
| lr              | 9.974e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0203943   |
| mse_q0          | 0.05434     |
| mse_q1          | 0.0176213   |
| mse_q2          | 0.00729537  |
| mse_q3          | 0.00458386  |
| param_norm      | 330.137     |
| samples         | 6288        |
| step            | 130         |
| vb              | 0.000378991 |
| vb_q0           | 0.00119861  |
| vb_q1           | 9.49255e-05 |
| vb_q2           | 3.67376e-05 |
| vb_q3           | 0.000168951 |
---------------------------------
---------------------------------
| grad_norm       | 0.150829    |
| lg_loss_scale   | 20.1355     |
| loss            | 0.0202304   |
| loss_q0         | 0.0522807   |
| loss_q1         | 0.0164199   |
| loss_q2         | 0.00706266  |
| loss_q3         | 0.00417033  |
| lr              | 9.972e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0202301   |
| mse_q0          | 0.0522799   |
| mse_q1          | 0.0164198   |
| mse_q2          | 0.00706263  |
| mse_q3          | 0.00417027  |
| param_norm      | 330.185     |
| samples         | 6768        |
| step            | 140         |
| vb              | 0.000241806 |
| vb_q0           | 0.000800021 |
| vb_q1           | 8.54109e-05 |
| vb_q2           | 3.55132e-05 |
| vb_q3           | 5.84549e-05 |
---------------------------------
---------------------------------
| grad_norm       | 0.151449    |
| lg_loss_scale   | 20.1455     |
| loss            | 0.0227272   |
| loss_q0         | 0.0624944   |
| loss_q1         | 0.0135641   |
| loss_q2         | 0.00700576  |
| loss_q3         | 0.00369139  |
| lr              | 9.97e-05    |
| lr_anneal_steps | 50000       |
| mse             | 0.0227266   |
| mse_q0          | 0.0624921   |
| mse_q1          | 0.0135641   |
| mse_q2          | 0.00700572  |
| mse_q3          | 0.00369128  |
| param_norm      | 330.229     |
| samples         | 7248        |
| step            | 150         |
| vb              | 0.000640225 |
| vb_q0           | 0.00228481  |
| vb_q1           | 6.79822e-05 |
| vb_q2           | 3.39577e-05 |
| vb_q3           | 0.000108605 |
---------------------------------
---------------------------------
| grad_norm       | 0.222338    |
| lg_loss_scale   | 20.1555     |
| loss            | 0.0212984   |
| loss_q0         | 0.0672006   |
| loss_q1         | 0.0146738   |
| loss_q2         | 0.00606746  |
| loss_q3         | 0.00361028  |
| lr              | 9.968e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0212975   |
| mse_q0          | 0.067197    |
| mse_q1          | 0.0146737   |
| mse_q2          | 0.00606743  |
| mse_q3          | 0.00361018  |
| param_norm      | 330.268     |
| samples         | 7728        |
| step            | 160         |
| vb              | 0.000863625 |
| vb_q0           | 0.00358704  |
| vb_q1           | 7.63071e-05 |
| vb_q2           | 3.0562e-05  |
| vb_q3           | 9.25295e-05 |
---------------------------------
---------------------------------
| grad_norm       | 0.167071    |
| lg_loss_scale   | 20.1655     |
| loss            | 0.0211601   |
| loss_q0         | 0.0685948   |
| loss_q1         | 0.0138175   |
| loss_q2         | 0.00575051  |
| loss_q3         | 0.00321539  |
| lr              | 9.966e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0211585   |
| mse_q0          | 0.0685876   |
| mse_q1          | 0.0138174   |
| mse_q2          | 0.00575048  |
| mse_q3          | 0.00321532  |
| param_norm      | 330.304     |
| samples         | 8208        |
| step            | 170         |
| vb              | 0.00165549  |
| vb_q0           | 0.00716604  |
| vb_q1           | 7.15566e-05 |
| vb_q2           | 2.90357e-05 |
| vb_q3           | 7.33364e-05 |
---------------------------------
---------------------------------
| grad_norm       | 0.134063    |
| lg_loss_scale   | 20.1755     |
| loss            | 0.0216306   |
| loss_q0         | 0.05677     |
| loss_q1         | 0.0139042   |
| loss_q2         | 0.00560283  |
| loss_q3         | 0.00286955  |
| lr              | 9.964e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.0216302   |
| mse_q0          | 0.0567688   |
| mse_q1          | 0.0139042   |
| mse_q2          | 0.0056028   |
| mse_q3          | 0.00286945  |
| param_norm      | 330.335     |
| samples         | 8688        |
| step            | 180         |
| vb              | 0.000398823 |
| vb_q0           | 0.00123094  |
| vb_q1           | 7.37369e-05 |
| vb_q2           | 2.80771e-05 |
| vb_q3           | 9.91133e-05 |
---------------------------------
---------------------------------
| grad_norm       | 0.125879    |
| lg_loss_scale   | 20.1855     |
| loss            | 0.0219687   |
| loss_q0         | 0.0646892   |
| loss_q1         | 0.0134182   |
| loss_q2         | 0.00535338  |
| loss_q3         | 0.00270506  |
| lr              | 9.962e-05   |
| lr_anneal_steps | 50000       |
| mse             | 0.021968    |
| mse_q0          | 0.0646866   |
| mse_q1          | 0.0134182   |
| mse_q2          | 0.00535336  |
| mse_q3          | 0.002705    |
| param_norm      | 330.363     |
| samples         | 9168        |
| step            | 190         |
| vb              | 0.000708534 |
| vb_q0           | 0.00259409  |
| vb_q1           | 7.01985e-05 |
| vb_q2           | 2.69441e-05 |
| vb_q3           | 5.87756e-05 |
---------------------------------
